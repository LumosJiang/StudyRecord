# 2024第四季度 论文学习记录

## 去雾+Mamba/visionRWKV

论文笔记内容：
① 解决什么问题？
② 创新点和方法是什么？
③ 结果和结论是什么？

泛读：
① 看论文摘要
② 看论文创新点（方法和图）
③ 看实验的表 看性能

精读：
看introduction relate-work experiment都看 做笔记
paper哪个地方最work的，怎么发现这个方法的

做实验：看相关方向顶会的一些baseline

## 1 Survey

### 1.1 Mamba

#### 1.1.1 [A Survey of Mamba](https://arxiv.org/abs/2408.01129)

##### 挑战

**Transformer**
① Transformer仍然面临着固有的局限性,尤其是注意力计算的二次计算复杂性导致推断过程非常耗时。
② 当涉及到从基于Transformer的模型生成响应或预测时,推理过程可能会很耗时。例如,语言模型的自动回归设计需要按顺序生成输出序列中的每个标记,这需要在每个步骤重复计算注意力得分,从而导致推理时间变慢。

**RNN**
① RNN在有效提取输入序列中的长程动态方面能力有限。当信息在连续的时间步中传递时,网络中权重的重复乘法会导致信息稀释或丢失。
② RNN以增量方式处理序列数据,这限制了它们的计算效率,因为每个时间步都依赖于前一个时间步。这使得并行计算对它们来说具有挑战性。
③ 传统的RNN缺乏内置的注意力机制,阻碍了网络对数据的关键部分进行选择性建模的能力。
④ 传统的RNN在非线性循环框架内运行,其中每个计算仅取决于先前的隐藏状态和当前输入。虽然这种格式允许RNN在自动回归推理期间快速生成输出,但它阻碍了它们充分利用GPU并行计算的能力,导致模型训练速度变慢。

**GNN**
这些模型面临一个被称为过度平滑的重大局限性(Chen等人,2020),尤其是在试图捕获高阶邻接信号时。

**CNN**
基于CNN的方法受局部感受野的限制,导致在捕捉全局和长距离语义时表现欠佳(Gu and Dao, 2023)。

**SSMs**
最传统的SSM是时间不变的,这意味着它们的𝐀、𝐁、𝐂和Δ与模型输入𝑥无关。这将限制上下文感知建模,从而导致SSM在某些任务中的性能较差,例如选择性复制。

**Mamba**
曼巴架构仍然面临一些挑战,例如内存损耗、对不同任务的泛化以及与基于Transformer的语言模型相比,捕捉复杂模式的能力较差。

##### 主要贡献

① 基于Mamba的模型的进展
② 使Mamba适应不同数据的技术
③ Mamba可以发挥优势的应用

具体来说,我们首先回顾了各种代表性深度学习模型的基础知识以及Mamba-1和Mamba-2的细节作为前期工作。然后,为了展示Mamba对人工智能的重要性,我们全面回顾了相关研究,重点关注Mamba模型的架构设计、数据适应性和应用。最后,我们讨论了当前的局限性,并探讨了各种有前途的研究方向,为未来的研究提供更深入的见解。

##### 知识点

**RNN**
① RNN是非线性循环模型,通过利用隐藏状态中存储的历史知识来有效地捕捉时间模式。
② 基于RNN的模型在捕捉时间动态方面取得了卓越的成果。
  
**GNN**
① GNN在通过消息传递机制捕获相邻关系方面展现出巨大的潜力,其中信息通过堆叠层在连接图上传播。

**Transformer**
① 自然语言处理中,Self-Attention使模型能够理解序列中不位置之间的关系。
②  In multi-head attention,输入序列由多组自我注意力模块并行处理。每个模块独立运行,执行与标准自我注意力机制完全相同的计算。然后,将每个模块的注意力权重相加,得到一个值向量的加权总和。通过这一聚合步骤,模型能够利用来自多个模块的信息,捕捉输入序列中的不同模式和关系。

**SSMs**
① 与RNN不同,SSMs是线性模型,具有关联性。
② 与仅能支持一种计算的RNN和Transformer不同,离散SSM具有线性特性,因此能够灵活地支持循环和卷积计算。

**Mamba**
*引入了三种基于结构化状态空间模型(Structured State Space Models)的创新技术*
  ①初始化策略(HIPPO)建立了一个连贯的隐藏状态矩阵,有效地促进了长程记忆。
  ②选择机制使SSM能够获得内容感知表示。
  ③Mamba设计了两种硬件感知计算算法,即并行关联扫描和内存重计算,以提高训练效率。

Mamba主要关注隐藏状态矩阵𝐀的初始化,以捕获复杂的时间依赖性。这是通过利用HiPPO理论(Gu et al., 2020)和创新的缩放勒让德测量(LegS)实现的,确保仔细考虑完整的历史背景,而不是有限的滑动窗口。
它在不同输入时间尺度下保持一致,并且计算速度很快 (Gu et al., 2020)。此外,它还具有有界梯度和近似误差,有助于参数学习过程。

Mamba 中时变选择机制的结构与 Transformer 中的注意力机制类似,即两者都根据输入及其投影执行操作,这使得 Mamba 的 SSM 能够实现灵活的内容感知建模。然而,它失去了与卷积的等效性,这对它的效率产生了负面影响。

并行关联扫描将模型训练的计算复杂度从$𝐎(𝑁^2𝑑)$降低到$𝐎(𝑁/𝑡)$。从本质上讲,扫描的核心是在给定的输入上构建一个平衡的二叉树,并从根开始遍历。换句话说,并行关联扫描首先从叶子到根遍历(即向上扫描),在树的内部节点创建部分和。然后,它反转遍历,从根向上移动,使用部分和构建整个扫描(即向下扫描)。
另一方面,Mamba利用传统的重计算方法来减少训练选择性SSM层所需的总体内存.

##### Mamba改进

① 块设计：整合、替换、修改
② 扫描模式:
平面扫描-Bidirectional Scan, Sweeping Scan, Continuous Scan, and Efficient Scan.
立体扫描-Hierarchical Scan, Spatiotemporal Scan, and Hybrid Scan.
③ 内存管理：存储器初始化、压缩和连接。

##### 机遇

Mamba具有Transformer的内容感知学习功能,同时能够根据输入长度线性扩展计算量,从而有效地捕捉长距离依赖关系,提高训练和推理的效率。

通过连接SSM和注意力,Mamba-2引入的SSD框架(Dao和Gu,2024)允许我们为Transformer和Mamba开发共享的词汇和技术库。

#### 1.1.2 [A Survey on Visual Mamba](https://arxiv.org/abs/2404.15956)

#### 1.1.3 [Visual Mamba: A Survey and New Outlooks](https://arxiv.org/abs/2404.18861)

### 1.2 dehazing

## 2 conference paper

### 2.1 Mamba/visionRWKv

#### 2.1.1 [Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures](https://arxiv.org/abs/2403.02308)

### 2.2 dehazing
